{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve,confusion_matrix,accuracy_score, auc\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pylab import rcParams\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPool1D\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import time \n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('F:creditcard.csv')\n",
    "#check on the size of the dataframe\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "df= shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We separate the feature vector from the classification result\n",
    "X = df.drop('Class',axis=1).values\n",
    "y = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "X -= X.min(axis=0)\n",
    "X /= X.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separation of data into training & test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "#print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "#print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "#print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "#print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\n",
    "#print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n",
    "\n",
    "#sm = SMOTE(random_state=2)\n",
    "#X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "#print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "#print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "##print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "#print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-613b2d212ad9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mx_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mx_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epoch = 50\n",
    "#batch_size = 256\n",
    "input_dim = X.shape[1] #num of columns, 30\n",
    "encoding_dim_1 = 27\n",
    "encoding_dim_2 = 24\n",
    "encoding_dim_3 = 21\n",
    "hidden_dim = 15\n",
    "learning_rate = 1e-7\n",
    "\n",
    "\n",
    "#Cross_Validate\n",
    "\n",
    "kf = StratifiedKFold(5, shuffle = True, random_state=42) # Use for Kfold Validation\n",
    "\n",
    "tn_summary=[]\n",
    "fp_summary=[]\n",
    "fn_summary=[]\n",
    "tp_summary=[]\n",
    "\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "for train, test in kf.split(X,y):\n",
    "    fold +=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_train = X[train]\n",
    "    y_train = y[train]\n",
    "    x_test = X[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    x_train, x_test = scaler.fit_transform(x_train), scaler.fit_transform(x_test) \n",
    "    \n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "    encoder = Dense(encoding_dim_1, activation=\"elu\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "    encoder = Dense(encoding_dim_2, activation = \"elu\")(encoder)\n",
    "    encoder = Dense(encoding_dim_3, activation = \"elu\")(encoder)\n",
    "\n",
    "    encoder = Dense(hidden_dim, activation=\"elu\")(encoder)\n",
    "\n",
    "    decoder = Dense(encoding_dim_3, activation = \"elu\")(encoder)\n",
    "    decoder = Dense(encoding_dim_2, activation = \"elu\")(decoder)\n",
    "    decoder = Dense(encoding_dim_1, activation = \"elu\")(decoder)\n",
    "    decoder = Dense(input_dim, activation=\"elu\")(decoder)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    \n",
    "    # Create the autoencoder model\n",
    "    autoencoder = Model(input_layer, decoder)\n",
    "    #Compile the autoencoder model\n",
    "    autoencoder.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    #Fit to train set and save to hist_auto for plotting purposes\n",
    "    hist_auto = autoencoder.fit(x_train, x_train,epochs=150,shuffle=True,validation_data=(x_test, x_test))\n",
    "    \n",
    "    \n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist_auto.history['loss'])\n",
    "    plt.plot(hist_auto.history['val_loss'])\n",
    "    plt.title('Autoencoder model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a separate model (encoder) in order to make encodings (first part of the autoencoder model)\n",
    "    encoder_model = Model(input_layer, encoder)\n",
    "    # Create a placeholder for an encoded input\n",
    "    encoded_input = Input(shape=(hidden_dim,))\n",
    "    \n",
    "    #Encode data set from above using the encoder\n",
    "    encoded_train_x = encoder_model.predict(x_train)\n",
    "    encoded_test_x = encoder_model.predict(x_test)\n",
    "    #Reshape data for the CNN model\n",
    "    encoded_train_x = encoded_train_x.reshape(encoded_train_x.shape[0],encoded_train_x.shape[1], 1)\n",
    "    encoded_test_x = encoded_test_x.reshape(encoded_test_x.shape[0], encoded_test_x.shape[1], 1)\n",
    "    encoded_train_x.shape\n",
    "\n",
    "    epochs = 50\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, 2, activation='relu', input_shape = encoded_train_x[1].shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(64, 2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss = 'binary_crossentropy',metrics=['accuracy'])\n",
    "    history = model.fit(encoded_train_x, y_train,epochs=epochs,validation_data=(encoded_test_x, y_test), verbose=1)\n",
    "    \n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Encoded model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    predictions_CNN_prob = model.predict(encoded_test_x)\n",
    "    predictions_CNN_prob = predictions_CNN_prob[:,0]\n",
    "    predictions_CNN_01 = np.where(predictions_CNN_prob > 0.5, 1, 0) #Turn probability to 0-1 binary output\n",
    "    #Print accuracy\n",
    "    acc_CNN = accuracy_score(y_test, predictions_CNN_01)\n",
    "    print('Overall accuracy of SMOTE - Deep Autoencoder - Convolutional Neural Network model:', acc_CNN)\n",
    "    #Print Area Under Curve\n",
    "    false_positive_rate, recall, thresholds = roc_curve(y_test,predictions_CNN_prob)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out (1-Specificity)')\n",
    "    plt.show()\n",
    "    #Print Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, predictions_CNN_01)\n",
    "    labels = ['No Default', 'Default']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d',cmap=\"Blues\", vmin = 0.2);\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"AUC(ROC): \" + str(metrics.roc_auc_score(y_test, predictions_CNN_01)))\n",
    "    print(\"Precision: \" + str(metrics.precision_score(y_test, predictions_CNN_01)))\n",
    "    print(\"Recall: \" + str(metrics.recall_score(y_test, predictions_CNN_01)))\n",
    "    print(\"F1 score: \" + str(metrics.f1_score(y_test, predictions_CNN_01)))\n",
    "\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, predictions_CNN_01).ravel()\n",
    "\n",
    "    print(\"False positives: \" + str(fp))\n",
    "    print(\"True positives: \" + str(tp))\n",
    "    print(\"False negatives: \" + str(fn))\n",
    "    print(\"True negatives: \" + str(tn))\n",
    "    \n",
    "    tn_summary.append(tn)\n",
    "    fp_summary.append(fp)\n",
    "    fn_summary.append(fn)\n",
    "    tp_summary.append(tp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\" Average tn = \" + str(Average(tn_summary)))\n",
    "print(\" Average fp = \" + str(Average(fp_summary)))\n",
    "print(\" Average fn = \"+ str(Average(fn_summary)))    \n",
    "print(\" Average tp = \"+ str(Average(tp_summary)))   \n",
    "\n",
    "\n",
    "Average_precision = Average(tp_summary)/(Average(tp_summary)+Average(fp_summary))\n",
    "Average_recall = Average(tp_summary)/(Average(tp_summary)+Average(fn_summary))\n",
    "Average_f1_score = 2*Average_precision*Average_recall/(Average_precision + Average_recall)\n",
    "\n",
    "print(\" Average Precision = \" + str(Average_precision))\n",
    "\n",
    "print(\" Average Recall = \" + str(Average_recall))\n",
    "\n",
    "print(\" Average F1 score = \" + str(Average_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tn=0\n",
    "total_fp=0\n",
    "total_fn=0\n",
    "total_tp=0\n",
    "\n",
    "for i in tn_summary:\n",
    "    total_tn = total_tn + i\n",
    "\n",
    "for j in fp_summary:\n",
    "    total_fp = total_fp + j\n",
    "    \n",
    "for x in fn_summary:\n",
    "    total_fn = total_fn + x\n",
    "    \n",
    "for y in tp_summary:\n",
    "    total_tp = total_tp + y\n",
    "\n",
    "print(\" Total tn: \" + str(total_tn))\n",
    "print(\" Total fp: \" + str(total_fp))\n",
    "print(\" Total fn: \" + str(total_fn))\n",
    "print(\" Total tp: \" + str(total_tp))\n",
    "\n",
    "\n",
    "Total_Precision = total_tp/(total_tp + total_fp)\n",
    "Total_Recall = total_tp/(total_tp+ total_fn)\n",
    "\n",
    "Total_F1_score = 2*Total_Precision*Total_Recall/(Total_Precision + Total_Recall)\n",
    "\n",
    "print(\" Total Precision = \" + str(Total_Precision))\n",
    "print(\" Total Recall = \" + str(Total_Recall))\n",
    "print(\" Total F1_score = \" + str(Total_F1_score))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "Total_time = (time.time()-start_time)\n",
    "\n",
    "print(Total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_time_hours = Total_time/3600\n",
    "\n",
    "print(Total_time_hours)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

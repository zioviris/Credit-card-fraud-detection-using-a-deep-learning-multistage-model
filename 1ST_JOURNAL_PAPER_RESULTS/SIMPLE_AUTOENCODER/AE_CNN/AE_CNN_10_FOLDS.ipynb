{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.bagging module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.ensemble.forest module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.ensemble. Anything that cannot be imported from sklearn.ensemble is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.utils.testing module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.utils. Anything that cannot be imported from sklearn.utils is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:144: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve,confusion_matrix,accuracy_score, auc\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from pylab import rcParams\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Flatten, Dense, Dropout, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPool1D\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, roc_curve, auc\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Lambda, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras import regularizers\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import time \n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset\n",
    "df = pd.read_csv('F:creditcard.csv')\n",
    "#check on the size of the dataframe\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "df= shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We separate the feature vector from the classification result\n",
    "X = df.drop('Class',axis=1).values\n",
    "y = df['Class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "X -= X.min(axis=0)\n",
    "X /= X.max(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20', 'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount', 'Class']\n"
     ]
    }
   ],
   "source": [
    "#Separation of data into training & test sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "#print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "#print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "#print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "#print(\"Number transactions y_test dataset: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Before OverSampling, counts of label '1': {}\".format(sum(y==1)))\n",
    "#print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y==0)))\n",
    "\n",
    "#sm = SMOTE(random_state=2)\n",
    "#X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n",
    "\n",
    "#print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "#print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "##print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "#print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold #1\n",
      "Train on 256326 samples, validate on 28481 samples\n",
      "Epoch 1/150\n",
      "256326/256326 [==============================] - 22s 86us/step - loss: 0.4510 - val_loss: 0.3521\n",
      "Epoch 2/150\n",
      "256326/256326 [==============================] - 19s 75us/step - loss: 0.3335 - val_loss: 0.3342\n",
      "Epoch 3/150\n",
      "256128/256326 [============================>.] - ETA: 0s - loss: 0.3050"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2c8f15d59f4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_squared_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m#Fit to train set and save to hist_auto for plotting purposes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mhist_auto\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    208\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 210\u001b[1;33m                                          verbose=0)\n\u001b[0m\u001b[0;32m    211\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                     \u001b[1;31m# Same labels assumed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    447\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'batch'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'size'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'begin'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3792\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3794\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1604\u001b[0m     \"\"\"\n\u001b[1;32m-> 1605\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1645\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1746\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 598\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    599\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mF:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_epoch = 50\n",
    "#batch_size = 256\n",
    "input_dim = X.shape[1] #num of columns, 30\n",
    "encoding_dim_1 = 27\n",
    "encoding_dim_2 = 24\n",
    "encoding_dim_3 = 21\n",
    "hidden_dim = 15\n",
    "learning_rate = 1e-7\n",
    "\n",
    "\n",
    "#Cross_Validate\n",
    "\n",
    "kf = StratifiedKFold(10, shuffle = True, random_state=42) # Use for Kfold Validation\n",
    "\n",
    "tn_summary=[]\n",
    "fp_summary=[]\n",
    "fn_summary=[]\n",
    "tp_summary=[]\n",
    "\n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "for train, test in kf.split(X,y):\n",
    "    fold +=1\n",
    "    print(f\"Fold #{fold}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    x_train = X[train]\n",
    "    y_train = y[train]\n",
    "    x_test = X[test]\n",
    "    y_test = y[test]\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    x_train, x_test = scaler.fit_transform(x_train), scaler.fit_transform(x_test) \n",
    "    \n",
    "    input_layer = Input(shape=(input_dim, ))\n",
    "\n",
    "    encoder = Dense(encoding_dim_1, activation=\"elu\", activity_regularizer=regularizers.l1(learning_rate))(input_layer)\n",
    "    encoder = Dense(encoding_dim_2, activation = \"elu\")(encoder)\n",
    "    encoder = Dense(encoding_dim_3, activation = \"elu\")(encoder)\n",
    "\n",
    "    encoder = Dense(hidden_dim, activation=\"elu\")(encoder)\n",
    "\n",
    "    decoder = Dense(encoding_dim_3, activation = \"elu\")(encoder)\n",
    "    decoder = Dense(encoding_dim_2, activation = \"elu\")(decoder)\n",
    "    decoder = Dense(encoding_dim_1, activation = \"elu\")(decoder)\n",
    "    decoder = Dense(input_dim, activation=\"elu\")(decoder)\n",
    "\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
    "    \n",
    "    # Create the autoencoder model\n",
    "    autoencoder = Model(input_layer, decoder)\n",
    "    #Compile the autoencoder model\n",
    "    autoencoder.compile(optimizer='adam',loss='mean_squared_error')\n",
    "    #Fit to train set and save to hist_auto for plotting purposes\n",
    "    hist_auto = autoencoder.fit(x_train, x_train,epochs=150,shuffle=True,validation_data=(x_test, x_test))\n",
    "    \n",
    "    \n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(hist_auto.history['loss'])\n",
    "    plt.plot(hist_auto.history['val_loss'])\n",
    "    plt.title('Autoencoder model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Create a separate model (encoder) in order to make encodings (first part of the autoencoder model)\n",
    "    encoder_model = Model(input_layer, encoder)\n",
    "    # Create a placeholder for an encoded input\n",
    "    encoded_input = Input(shape=(hidden_dim,))\n",
    "    \n",
    "    #Encode data set from above using the encoder\n",
    "    encoded_train_x = encoder_model.predict(x_train)\n",
    "    encoded_test_x = encoder_model.predict(x_test)\n",
    "    #Reshape data for the CNN model\n",
    "    encoded_train_x = encoded_train_x.reshape(encoded_train_x.shape[0],encoded_train_x.shape[1], 1)\n",
    "    encoded_test_x = encoded_test_x.reshape(encoded_test_x.shape[0], encoded_test_x.shape[1], 1)\n",
    "    encoded_train_x.shape\n",
    "\n",
    "    epochs = 50\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32, 2, activation='relu', input_shape = encoded_train_x[1].shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv1D(64, 2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPool1D(2))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=0.0001), loss = 'binary_crossentropy',metrics=['accuracy'])\n",
    "    history = model.fit(encoded_train_x, y_train,epochs=epochs,validation_data=(encoded_test_x, y_test), verbose=1)\n",
    "    \n",
    "    # Summarize history for loss\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Encoded model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "    predictions_CNN_prob = model.predict(encoded_test_x)\n",
    "    predictions_CNN_prob = predictions_CNN_prob[:,0]\n",
    "    predictions_CNN_01 = np.where(predictions_CNN_prob > 0.5, 1, 0) #Turn probability to 0-1 binary output\n",
    "    #Print accuracy\n",
    "    acc_CNN = accuracy_score(y_test, predictions_CNN_01)\n",
    "    print('Overall accuracy of SMOTE - Deep Autoencoder - Convolutional Neural Network model:', acc_CNN)\n",
    "    #Print Area Under Curve\n",
    "    false_positive_rate, recall, thresholds = roc_curve(y_test,predictions_CNN_prob)\n",
    "    roc_auc = auc(false_positive_rate, recall)\n",
    "    plt.figure()\n",
    "    plt.title('Receiver Operating Characteristic (ROC)')\n",
    "    plt.plot(false_positive_rate, recall, 'b', label = 'AUC = %0.3f' %roc_auc)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.plot([0,1], [0,1], 'r--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.0])\n",
    "    plt.ylabel('Recall')\n",
    "    plt.xlabel('Fall-out (1-Specificity)')\n",
    "    plt.show()\n",
    "    #Print Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, predictions_CNN_01)\n",
    "    labels = ['No Default', 'Default']\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.heatmap(cm,xticklabels=labels, yticklabels=labels, annot=True, fmt='d',cmap=\"Blues\", vmin = 0.2);\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Class')\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"AUC(ROC): \" + str(metrics.roc_auc_score(y_test, predictions_CNN_01)))\n",
    "    print(\"Precision: \" + str(metrics.precision_score(y_test, predictions_CNN_01)))\n",
    "    print(\"Recall: \" + str(metrics.recall_score(y_test, predictions_CNN_01)))\n",
    "    print(\"F1 score: \" + str(metrics.f1_score(y_test, predictions_CNN_01)))\n",
    "\n",
    "    tn, fp, fn, tp = metrics.confusion_matrix(y_test, predictions_CNN_01).ravel()\n",
    "\n",
    "    print(\"False positives: \" + str(fp))\n",
    "    print(\"True positives: \" + str(tp))\n",
    "    print(\"False negatives: \" + str(fn))\n",
    "    print(\"True negatives: \" + str(tn))\n",
    "    \n",
    "    tn_summary.append(tn)\n",
    "    fp_summary.append(fp)\n",
    "    fn_summary.append(fn)\n",
    "    tp_summary.append(tp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\" Average tn = \" + str(Average(tn_summary)))\n",
    "print(\" Average fp = \" + str(Average(fp_summary)))\n",
    "print(\" Average fn = \"+ str(Average(fn_summary)))    \n",
    "print(\" Average tp = \"+ str(Average(tp_summary)))   \n",
    "\n",
    "\n",
    "Average_precision = Average(tp_summary)/(Average(tp_summary)+Average(fp_summary))\n",
    "Average_recall = Average(tp_summary)/(Average(tp_summary)+Average(fn_summary))\n",
    "Average_f1_score = 2*Average_precision*Average_recall/(Average_precision + Average_recall)\n",
    "\n",
    "print(\" Average Precision = \" + str(Average_precision))\n",
    "\n",
    "print(\" Average Recall = \" + str(Average_recall))\n",
    "\n",
    "print(\" Average F1 score = \" + str(Average_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tn=0\n",
    "total_fp=0\n",
    "total_fn=0\n",
    "total_tp=0\n",
    "\n",
    "for i in tn_summary:\n",
    "    total_tn = total_tn + i\n",
    "\n",
    "for j in fp_summary:\n",
    "    total_fp = total_fp + j\n",
    "    \n",
    "for x in fn_summary:\n",
    "    total_fn = total_fn + x\n",
    "    \n",
    "for y in tp_summary:\n",
    "    total_tp = total_tp + y\n",
    "\n",
    "print(\" Total tn: \" + str(total_tn))\n",
    "print(\" Total fp: \" + str(total_fp))\n",
    "print(\" Total fn: \" + str(total_fn))\n",
    "print(\" Total tp: \" + str(total_tp))\n",
    "\n",
    "\n",
    "Total_Precision = total_tp/(total_tp + total_fp)\n",
    "Total_Recall = total_tp/(total_tp+ total_fn)\n",
    "\n",
    "Total_F1_score = 2*Total_Precision*Total_Recall/(Total_Precision + Total_Recall)\n",
    "\n",
    "print(\" Total Precision = \" + str(Total_Precision))\n",
    "print(\" Total Recall = \" + str(Total_Recall))\n",
    "print(\" Total F1_score = \" + str(Total_F1_score))\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "Total_time = (time.time()-start_time)\n",
    "\n",
    "print(Total_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_time_hours = Total_time/3600\n",
    "\n",
    "print(Total_time_hours)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
